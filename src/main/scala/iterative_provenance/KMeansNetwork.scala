package iterative_provenance

import org.apache.spark.{SparkConf, SparkContext}

// jteoh: Mix of gulzar's work, ml.KMeansExample, and gist @ https://gist.github.com/umbertogriffo/b599aa9b9a156bb1e8775c8cdbfb688a
object KMeansNetwork extends BaselineApp {
  
  def parseVector(line: String): Array[Double] =
    line.split(" ").map(_.toDouble)
  
  def parseAutogeneratedVector(line: String): Array[Double] =
  // ex: 0 1:313.37 2:-307.07 3:651.23 4:728.55 5:-254.32 6:676.80 7:-136.03 8:1875.63 9:-859.88
  // format: rowID, colId:value
    line.split(" ").drop(1).map(_.split(":")(1).toDouble)
  
  def squaredDistance(p: Array[Double],
                      center: Array[Double]): Double= {
    p.zip(center).map(s => s._1 - s._2).map(s => s * s).sum
  }
  
  def closestCenter(p: Array[Double],
                    centers: Array[Array[Double]]): Int = {
    centers.indices.minBy(i => squaredDistance(p, centers(i)))
    /**
    var bestIndex = 0
    var closest = Double.PositiveInfinity
    
    for (i <- centers.indices) {
      val tempDist = squaredDistance(p, centers(i))
      if (tempDist < closest) {
        closest = tempDist
        bestIndex = i
      }
    }
    bestIndex
     **/
  }
  
  // args: dataset, number of clusters, convergence distance, maxIterations, seed
  // defaults: (local file), 3,
  def run(args: Array[String]): Unit = {
    val sparkConf = new SparkConf()
    
    sparkConf.setAppName("WeatherBaseline-titian")
    val runningOnCluster = System.getProperty("user.name") == "plsys-jia"
    if(!runningOnCluster) {
      sparkConf.setMaster("local[*]")
      sparkConf.set("spark.eventLog.enabled", "true")
    } else {
      assert(args.length > 0, "Cluster execution requires at least the dataset as an argument")
    }
    sparkConf.setAppName("KMeansIterativeProv")
    
    
    val sc = new SparkContext(sparkConf)
    sc.setLogLevel("WARN")
    
    val useDatasetSchema = args.headOption.filter(_.nonEmpty).getOrElse("generated")
    val data = useDatasetSchema match {
      case "generated" =>
        val lines = sc.textFile("/Users/jteoh/Code/FineGrainedDataProvenance/KMeans_1M_3")
        //"/Users/malig/workspace/git/ADSpark/src/main/resources/delivery_data.txt"
        lines.map(s => parseAutogeneratedVector(s)).cache() // careful: caching will break later rbk
      case "spark-default" =>
        val lines = sc.textFile("/Users/jteoh/Code/distributions/spark-2.2.0-bin-hadoop2.7/data" +
                                 "/mllib/kmeans_data.txt")
        //"/Users/malig/workspace/git/ADSpark/src/main/resources/delivery_data.txt"
        lines.map(s => parseVector(s)).cache() // careful: caching will break later rbk
      // because of double-computation.
      case "unknown" =>
        // local testing only, data format is different...
        // this is a larger dataset to test with.
        sc.textFile(args.headOption
                        .getOrElse("/Users/jteoh/Code/FineGrainedDataProvenance/kmeans_sample_data"))
          .map(line => line.split(','))
          .map(pair => Array(pair(3).toDouble, pair(4).toDouble))
          .filter(point => !point.contains(0.0))
    }
    data.setName("data")
    
    
    // TODO: data should be persisted, but then taps won't work later in reduceByKey
    // data.persist()
    
    val K = args.lift(1).map(_.toInt).getOrElse(3) // number of clusters
    val convergeDist = args.lift(2).map(_.toDouble).getOrElse(0.1) // based on gist
    val maxIter = args.lift(3).map(_.toInt).getOrElse(20) // 20 iterations based on ml.KMeans
    val seed = args.lift(4).map(_.toInt).getOrElse(42)
    
    val kPoints: Array[Array[Double]] = data.takeSample(withReplacement = false, K, seed)
    //val kPoints: Array[Array[Double]] = Array(Array(1.0, 2.0), Array(2.0, 3.0), Array(3.0, 4.0))
    var tempDist = Double.PositiveInfinity // used to measure convergence rate
    var iteration = 1
    
    
    // Key differences between simple and library implementation:
    // (1) Initialization options - uses simple random as opposed to optimized version. Should be fine for our use case.
    // (2) broadcasting the centers rather than serializing tasks (should be negligible for small K)
    // (3) mapPartitions to optimize the cluster-sum+count outputs, as opposed to relying solely on reduceByKey
    
    // Detailed description of how it works in distributed fashion:
    // (0) initialize center points (typically done with KMeansParallel approach, but we use random for simplicity)
    // each iteration (until max iterations or convergence as defined in 7.1)
    // (1) broadcast the center points
    // (2) create a costAccumulator (double) - (this isn't strictly required though, just to measure accuracy of result)
    // (3) map data per partition:
    // (3.0): initialize (a) sum-vectors for each center, and (b) count for each center.
    // (3.1) for each point, find its closest center and the cost (distance)
    // (3.2) add cost to the cost costAccumulator
    // (3.3) for the closest center: add to sum (using axpy) and increment count (from 3.0)
    //    y += a*x, with a = 1, x = point-vector, y = sum (in other words, sum the vectors)
    // (3.4) produce (center_index, (sum_index, count_index)) for each center with non-zero count
    // (4) reduceByKey: for each center: add the sum-vectors and the counts together.
    // (5) collectAsMap: these will be used to later calculate the new centers
    // (6) delete broadcast variable from (1)
    // (7) update the collected map to compute centers (sum-vector / scalar count)
    // (7.1) checking if distance between old and new center is within epsilon^2 (convergence)
    // (8) Update cost (from accumulator) and iteration counter
    // (8.1) cost is actually never really used, except for one print at the end of all execution
    
    // JTEOH
    // lc.setCaptureLineage(true)
    
    
    while (tempDist > convergeDist && iteration <= maxIter) {
      
      // find the closest center for each point. also print out (point, 1) similar to wordcount...
      val closest = data.map(p => (closestCenter(p, kPoints), (p, 1)))
                                      .setName(s"Closest @ iter $iteration")
      
      // for each center, sum the points and the counts so we can average them later.
      val pointStats = closest.reduceByKey {
        case ((p1, c1), (p2, c2)) =>
          (p1.zip(p2).map(s => s._1 + s._2), c1 + c2)
      }.setName(s"PointStats @ iter $iteration")
      
      // compute the average point, done by dividing the summed point (vector) by count.
      val newPointsRDD = pointStats
        .map { pair =>
          (pair._1, pair._2._1.map(s => s / pair._2._2))
        }.setName(s"newPoints @ iter $iteration")
      
      val newPoints = newPointsRDD.collectAsMap()
      
      
      println("New centers: ")
      println(newPoints.mapValues(_.mkString("[", ",", "]")).mkString(","))
     
      // Calculate the rate of convergence (how much our centers moved since previous iteration)
      tempDist = 0.0
      for (i <- 0 until K) {
        tempDist = tempDist + squaredDistance(kPoints(i), newPoints(i))
      }
      
      
      // Update our centers with the newly calculated ones.
      for (newP <- newPoints) {
        kPoints(newP._1) = newP._2
      }
      
      println(s"Finished iteration $iteration (delta = $tempDist)")
      iteration += 1
      
      
    }
    
    println("Final centers:")
    println("====")
    
    kPoints.zipWithIndex.foreach({ case (center, index) =>
      println(index + " " + center.mkString("[", ",", "]"))
      println("===")
    })
    
    
    sc.stop()
  }
}

